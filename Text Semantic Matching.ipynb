{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_8.data import load_vocab, load_lcqmc_data\n",
    "# 加载训练集，验证集，测试集\n",
    "train_data, dev_data, test_data = load_lcqmc_data('lcqmc')\n",
    "# 加载词表\n",
    "word2id_dict = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.io import Dataset\n",
    "\n",
    "class LCQMCDataset(Dataset):\n",
    "    def __init__(self, data, word2id_dict):\n",
    "        # 词表\n",
    "        self.word2id_dict = word2id_dict\n",
    "        # 数据\n",
    "        self.examples = data\n",
    "        # ['CLS']的id，占位符\n",
    "        self.cls_id = self.word2id_dict['[CLS]']\n",
    "        # ['SEP']的id，句子的分隔\n",
    "        self.sep_id = self.word2id_dict['[SEP]']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回单条样本\n",
    "        example = self.examples[idx]\n",
    "        text, segment, label = self.words_to_id(example)\n",
    "        return text, segment, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 返回样本的个数\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def words_to_id(self, example):\n",
    "        text_a, text_b, label = example\n",
    "        # text_a 转换成id的形式\n",
    "        input_ids_a = [self.word2id_dict[item] if item in self.word2id_dict else self.word2id_dict['[UNK]'] for item in text_a]\n",
    "        # text_b 转换成id的形式\n",
    "        input_ids_b = [self.word2id_dict[item] if item in self.word2id_dict else self.word2id_dict['[UNK]'] for item in text_b]\n",
    "        # 加入[CLS], [SEP]\n",
    "        input_ids = [self.cls_id] + input_ids_a + [self.sep_id] + input_ids_b + [self.sep_id]\n",
    "        # 对句子text_a,text_b做id的区分，进行的分隔\n",
    "        segment_ids = [0]*(len(input_ids_a)+2) + [1]*(len(input_ids_b)+1)\n",
    "        return input_ids, segment_ids, int(label)\n",
    "    \n",
    "    @property\n",
    "    def label_list(self):\n",
    "        # 0表示不相似，1表示相似\n",
    "        return ['0', '1']\n",
    "\n",
    "# 加载训练集\n",
    "train_dataset = LCQMCDataset(train_data, word2id_dict)\n",
    "# 加载验证集\n",
    "dev_dataset = LCQMCDataset(dev_data, word2id_dict)\n",
    "# 加载测试集\n",
    "test_dataset = LCQMCDataset(test_data, word2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.io import DataLoader\n",
    "import paddle\n",
    "\n",
    "def collate_fn(batch_data, pad_val=0, max_seq_len=512):\n",
    "    input_ids, segment_ids, labels = [], [], []\n",
    "    max_len = 0\n",
    "    # print(batch_data)\n",
    "    for example in batch_data:\n",
    "        input_id, segment_id, label = example\n",
    "        # 对数据序列进行截断\n",
    "        input_ids.append(input_id[:max_seq_len])\n",
    "        segment_ids.append(segment_id[:max_seq_len])\n",
    "        labels.append(label)\n",
    "        # 保存序列最大长度\n",
    "        max_len = max(max_len, len(input_id))\n",
    "    # 对数据序列进行填充至最大长度\n",
    "    for i in range(len(labels)):\n",
    "        input_ids[i] = input_ids[i] + [pad_val]*(max_len-len(input_ids[i]))\n",
    "        segment_ids[i] = segment_ids[i] + [pad_val]*(max_len-len(segment_ids[i]))\n",
    "    return (paddle.to_tensor(input_ids), paddle.to_tensor(segment_ids)), paddle.to_tensor(labels)\n",
    "\n",
    "batch_size = 32\n",
    "# 构建训练集，验证集，测试集的dataloader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# 打印输出一条mini-batch的数据\n",
    "for idx, item in enumerate(train_loader):\n",
    "    if idx == 0:\n",
    "        print(item)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "# 输入编码\n",
    "class WordEmbedding(nn.Layer):\n",
    "    def __init__(self, vocab_size, emb_size, padding_idx=0):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        # Embedding的维度\n",
    "        self.emb_size = emb_size\n",
    "        # 使用随机正态（高斯）分布初始化 embedding\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size, padding_idx=padding_idx,\n",
    "                                           weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(0.0, emb_size ** -0.5)))\n",
    "        \n",
    "    def forward(self, word):\n",
    "        word_emb = self.emb_size ** 0.5 * self.word_embedding(word)\n",
    "        return word_emb\n",
    "    \n",
    "paddle.seed(2023)\n",
    "# 构造一个输入\n",
    "X = paddle.to_tensor([1, 0, 2])\n",
    "# 表示构造的输入编码的词汇表的大小是10，每个词的维度是4\n",
    "word_embed = WordEmbedding(10, 4)\n",
    "print(\"输入编码为：{}\".format(X.numpy()))\n",
    "word_out = word_embed(X)\n",
    "print(\"输出编码为：{}\".format(word_out.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分段编码\n",
    "class SegmentEmbedding(nn.Layer):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SegmentEmbedding, self).__init__()\n",
    "        # Embedding的维度\n",
    "        self.emb_size = emb_size\n",
    "        # 分段编码\n",
    "        self.seg_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "\n",
    "    def forward(self, word):\n",
    "        seg_embedding = self.seg_embedding(word)\n",
    "        return seg_embedding\n",
    "    \n",
    "paddle.seed(2023)\n",
    "# 构造一个输入,0表示第0句的token，1表示第1句的token\n",
    "X = paddle.to_tensor([0, 0, 1, 1])\n",
    "word_embed = SegmentEmbedding(2, 4)\n",
    "print(\"输入编码为：{}\".format(X.numpy()))\n",
    "word_out = word_embed(X)\n",
    "print(\"输出为：{}\".format(word_out.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "# position_size 为句子划分成字符或者词的长度，hidden_size为词向量的维度。\n",
    "def get_sinusoid_encoding(position_size, hidden_size):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "\n",
    "    def cal_angle(pos, hidden_idx):\n",
    "        # 公式里的 i = hid_idx // 2\n",
    "        return pos / np.power(10000, 2*(hidden_idx//2)/hidden_size)\n",
    "    \n",
    "    def get_posi_angle_vec(pos):\n",
    "        return [cal_angle(pos, hidden_j) for hidden_j in range(hidden_size)]\n",
    "    \n",
    "    sinusoid = np.array([get_posi_angle_vec(pos_i) for pos_i in range(position_size)])\n",
    "    # dim 2i 偶数正弦\n",
    "    # 从0开始，每隔2间隔取余弦\n",
    "    sinusoid[:, 0::2] = np.sin(sinusoid[:, 0::2])\n",
    "    # dim 2i 1  奇数余弦\n",
    "    # 从1开始，每隔2间隔取余弦\n",
    "    sinusoid[:, 1::2] = np.cos(sinusoid[:, 1::2])\n",
    "    # position_size × hidden_size  得到每一个词的位置向量\n",
    "    return sinusoid.astype(\"float32\")\n",
    "\n",
    "paddle.seed(2023)\n",
    "position_size = 4\n",
    "hidden_size = 3\n",
    "encoding_vec = get_sinusoid_encoding(position_size=position_size, hidden_size=hidden_size)\n",
    "print(\"位置编码的输出：{}\".format(encoding_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Layer):\n",
    "    def __init__(self, max_length, emb_size):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        # 使用三角函数初始化Embedding\n",
    "        self.pos_encoder = nn.Embedding(num_embeddings=max_length, embedding_dim=self.emb_size,\n",
    "                                        weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Assign(get_sinusoid_encoding(max_length, self.emb_size))))\n",
    "        \n",
    "    def forward(self, pos):\n",
    "        pos_emb = self.pos_encoder(pos)\n",
    "        # 关闭位置编码的梯度更新\n",
    "        pos_emb.stop_gradient = True\n",
    "        return pos_emb\n",
    "    \n",
    "paddle.seed(2023)\n",
    "out = paddle.randint(low=0, high=5, shape=[3])\n",
    "print('输入向量为：{}'.format(out.numpy()))\n",
    "pos_embed = PositionalEmbedding(4, 5)\n",
    "pos_out = pos_embed(out)\n",
    "print('位置编码的输出为：{}'.format(pos_out.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curve(size, y):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(np.arange(size), y[0, :, 4:5].numpy(), color='#E20079', linestyle='-')\n",
    "    plt.plot(np.arange(size), y[0, :, 5:6].numpy(), color='#3D3D3F', linestyle='--')\n",
    "    plt.plot(np.arange(size), y[0, :, 6:7].numpy(), color='#8e004D', linestyle='-.')\n",
    "    plt.plot(np.arange(size), y[0, :, 7:8].numpy(), color='#284279', linestyle=':')\n",
    "    plt.legend([\"dim %d\"%p for p in [4, 5, 6, 7]], fontsize='large')\n",
    "    plt.savefig('att-vis2.pdf')\n",
    "\n",
    "model = PositionalEmbedding(emb_size=20, max_length=5000)\n",
    "# 生成0~99这100个数，表示0~99这100个位置\n",
    "size = 100\n",
    "X = paddle.arange((size)).reshape([1, size])\n",
    "# print(X)\n",
    "# 对这100个位置进行编码，得到每个位置的向量表示\n",
    "# y: [1,100,20]\n",
    "y = model(X)\n",
    "# print(y)\n",
    "# 把这100个位置的第4，5，6，7列的数据可视化出来\n",
    "plot_curve(size=size, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbeddings(nn.Layer):\n",
    "    \"\"\"\n",
    "    包括输入编码，分段编码，位置编码\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size=768, hidden_dropout_prob=0.1, position_size=512, segment_size=2):\n",
    "        super(TransformerEmbeddings, self).__init__()\n",
    "        # 输入编码向量\n",
    "        self.word_embeddings = WordEmbedding(vocab_size=vocab_size, emb_size=hidden_size)\n",
    "        # 位置编码向量\n",
    "        self.position_embeddings = PositionalEmbedding(max_length=position_size, emb_size=hidden_size)\n",
    "        # 分段编码\n",
    "        self.segment_embeddings = SegmentEmbedding(vocab_size=segment_size, emb_size=hidden_size)\n",
    "        # 层规范化\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        # Dropout操作\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids=None, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            # 初始化全1的向量，比如[1,1,1,1]\n",
    "            ones = paddle.ones_like(input_ids, dtype=\"int64\")\n",
    "            # 累加输入,求出序列前K个的长度,比如[1,2,3,4]\n",
    "            seq_length = paddle.cumsum(ones, axis=-1)\n",
    "            # position id的形式：比如[0, 1, 2, 3]\n",
    "            position_ids = seq_length - ones\n",
    "            position_ids.stop_gradient = True\n",
    "        # 输入编码\n",
    "        input_embeddings = self.word_embeddings(input_ids)\n",
    "        # 分段编码\n",
    "        segment_embeddings = self.segment_embeddings(segment_ids)\n",
    "        # 位置编码\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # 输入张量, 分段张量，位置张量进行叠加\n",
    "        embeddings = input_embeddings + segment_embeddings + position_embeddings\n",
    "        # 层规范化\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        # DropOut\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer组块: 多头注意力层、加与规范化层、前馈层、加与规范化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "\n",
    "class AddNorm(nn.Layer):\n",
    "    \"\"\"加与规范化\"\"\"\n",
    "    def __init__(self, size, dropout_rate):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, H):\n",
    "        \"\"\"\n",
    "            X：表示被包裹的非线性层的输入\n",
    "            H：表示被包裹的非线性层的输出\n",
    "        \"\"\"\n",
    "        H = X+self.dropout(H)\n",
    "        return self.layer_norm(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "class PositionwiseFFN(nn.Layer):\n",
    "    \"\"\"逐位前馈层\"\"\"\n",
    "    def __init__(self, input_size, mid_size, dropout=0.1):\n",
    "        super(PositionwiseFFN, self).__init__()\n",
    "        self.W_1 = nn.Linear(input_size, mid_size)\n",
    "        self.W_2 = nn.Linear(mid_size, input_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.W_2(self.dropout(F.relu(self.W_1(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Layer):\n",
    "    def __init__(self, input_size, head_num, ffn_size, dropout=0.1, attn_dropout=None, act_dropout=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # 输入数据的维度\n",
    "        self.input_size = input_size\n",
    "        # 多头自注意力多头的个数\n",
    "        self.head_num = head_num\n",
    "        # 逐位前馈层的大小\n",
    "        self.ffn_size = ffn_size\n",
    "        # 加与规范化里面 Dropout的参数\n",
    "        self.dropout = dropout\n",
    "        # 多头注意力里面的 Dropout参数\n",
    "        self.attn_dropout = dropout if attn_dropout is None else attn_dropout\n",
    "        # 逐位前馈层里面的 Dropout参数\n",
    "        self.act_dropout = dropout if act_dropout is None else act_dropout\n",
    "        # 多头自注意力机制\n",
    "        self.multi_head_attention = nn.MultiHeadAttention(embed_dim=self.input_size, num_heads=self.head_num, dropout=self.attn_dropout, need_weights=True)\n",
    "        # 逐位前馈层\n",
    "        self.ffn = PositionwiseFFN(self.input_size, self.ffn_size, self.act_dropout)\n",
    "        # 加与规范化\n",
    "        self.addnorm = AddNorm(size=self.input_size, dropout_rate=self.dropout)\n",
    "\n",
    "    def forward(self, X, src_mask=None):\n",
    "        # 多头注意力\n",
    "        X_atten, atten_weights = self.multi_head_attention(X, attn_mask=src_mask)\n",
    "        # 加与规范化\n",
    "        X = self.addnorm(X, X_atten)\n",
    "        # 前馈层\n",
    "        X_ffn = self.ffn(X)\n",
    "        # 加与规范化\n",
    "        X = self.addnorm(X, X_ffn)\n",
    "        return X, atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型汇总\n",
    "class Model_Transformer(nn.Layer):\n",
    "    def __init__(self, vocab_size, n_block=2, hidden_size=768, heads_num=12, intermediate_size=3072, hidden_dropout=0.1,\n",
    "                 attention_dropout=0.1, act_dropout=0, position_size=512, num_classes=2, padding_idx=0):\n",
    "        super(Model_Transformer, self).__init__()\n",
    "        # 词表大小\n",
    "        self.vocab_size = vocab_size\n",
    "        # Transformer的编码器的数目\n",
    "        self.n_block = n_block\n",
    "        # 每个词映射成稠密向量的维度\n",
    "        self.hidden_size = hidden_size\n",
    "        # 多头注意力的个数\n",
    "        self.heads_num = heads_num\n",
    "        # 逐位前馈层的的维度\n",
    "        self.intermediate_size = intermediate_size\n",
    "        # Embedding层的 Dropout\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        # 多头注意力的dropout的 dropout参数\n",
    "        self.attention_dropout = attention_dropout\n",
    "        # 位置编码的大小 position_size\n",
    "        self.position_size = position_size\n",
    "        # 类别数\n",
    "        self.num_classes = num_classes\n",
    "        # 逐位前馈层的dropout\n",
    "        self.act_dropout = act_dropout\n",
    "        # [PAD]字符的ID\n",
    "        self.padding_idx = padding_idx\n",
    "        # 实例化输入编码，分段编码和位置编码\n",
    "        self.embeddings = TransformerEmbeddings(vocab_size=self.vocab_size, hidden_size=self.hidden_size, hidden_dropout_prob=self.hidden_dropout, position_size=self.position_size)\n",
    "        # 实例化Transformer的编码器\n",
    "        self.layers = nn.LayerList([])\n",
    "        for i in range(n_block):\n",
    "            encoder_layer = TransformerBlock(input_size=hidden_size, head_num=heads_num, ffn_size=intermediate_size, dropout=hidden_dropout,\n",
    "                                             attn_dropout=attention_dropout, act_dropout=act_dropout)\n",
    "            self.layers.append(encoder_layer)\n",
    "        # 全连接层\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        # 双曲正切激活函数\n",
    "        self.activation = nn.Tanh()\n",
    "        # 最后一层分类器\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, inputs, position_ids=None, attention_mask=None):\n",
    "        input_ids, segment_ids = inputs\n",
    "        # 构建Mask矩阵，把Pad的位置即input_ids中为0的位置设置为True,非0的位置设置为False\n",
    "        if attention_mask is None:\n",
    "            attention_mask = paddle.unsqueeze((input_ids == self.padding_idx).astype(\"float32\") * -1e9, axis=[1,2])\n",
    "        # 抽取特征向量\n",
    "        embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, segment_ids=segment_ids)\n",
    "        sequence_output = embedding_output\n",
    "        self._attention_weights = []\n",
    "        # Transformer的输出和注意力权重的输出\n",
    "        for i, encoder_layer in enumerate(self.layers):\n",
    "            sequence_output, atten_weights = encoder_layer(sequence_output, src_mask=attention_mask)\n",
    "            self._attention_weights.append(atten_weights)\n",
    "        # 选择第0个位置的向量作为句向量\n",
    "        first_token_tensor = sequence_output[:, 0]\n",
    "        # 输出层\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        # 句子级别的输出经过分类器\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def attention_weight(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndl_8_2 import Accuracy, RunnerV3\n",
    "import os\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "paddle.seed(2023)\n",
    "heads_num = 4\n",
    "epochs = 3\n",
    "vocab_size = 21128\n",
    "num_classes = 2\n",
    "padding_idx = word2id_dict['[PAD]']\n",
    "# 注意力多头的数目\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 评估的时候采用准确率指标\n",
    "metric = Accuracy()\n",
    "model = Model_Transformer(vocab_size=vocab_size, n_block=1, num_classes=num_classes, padding_idx=padding_idx, heads_num=heads_num)\n",
    "# 排除所有的偏置和LayerNorm的参数\n",
    "decay_params = [p.name for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in [\"bias\", \"norm\"])]\n",
    "\n",
    "# 定义Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=5E-5, parameters=model.parameters(), weight_decay=0.0, apply_decay_param_fun=lambda x:x in decay_params)\n",
    "\n",
    "runner = RunnerV3(model=model, optimizer=optimizer, loss_fn=criterion, metric=metric)\n",
    "save_path = './checkpoint/model_best03.pdparams'\n",
    "runner.train(train_loader=train_loader, dev_loader=dev_loader, num_epochs=epochs, log_steps=100, eval_steps=500, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndl_8_2 import plot\n",
    "\n",
    "plot(runner=runner, fig_name='att-loss-acc3.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './checkpoint/model_best03.pdparams'\n",
    "runner.load_model(model_path=model_path)\n",
    "accuracy, _ = runner.evaluate(test_loader)\n",
    "print(f\"Evaluate on test set, Accuracy:{accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './checkpoint/model_best03.pdparams'\n",
    "runner.load_model(model_path=model_path)\n",
    "text_a = '电脑怎么录像'\n",
    "text_b = '如何在计算机上录视频'\n",
    "# [CLS]转换成id\n",
    "cls_id = word2id_dict['[CLS]']\n",
    "# [SEP]转换成id\n",
    "sep_id = word2id_dict['[SEP]']\n",
    "# text_a转换成id的形式\n",
    "input_ids_a = [word2id_dict[item] if item in word2id_dict else word2id_dict['[UNK]']\n",
    "               for item in text_a]\n",
    "# text_b转换成id的形式\n",
    "input_ids_b = [word2id_dict[item] if item in word2id_dict else word2id_dict['[UNK]']\n",
    "               for item in text_b]\n",
    "# 两个句子拼接成id的形式\n",
    "input_ids = [cls_id] + input_ids_a +[sep_id] + input_ids_b + [sep_id]\n",
    "# 分段id的形式\n",
    "segment_ids = [0]*(len(input_ids_a)+2) + [1]*(len(input_ids_b)+1)\n",
    "# 转换成Tensor张量\n",
    "input_ids = paddle.to_tensor([input_ids])\n",
    "segment_ids = paddle.to_tensor([segment_ids])\n",
    "inputs = [input_ids, segment_ids]\n",
    "# 模型预测\n",
    "logits = runner.predict(inputs)\n",
    "# 取概率最大的索引\n",
    "label_id = paddle.argmax(logits, axis=1).numpy()[0]\n",
    "# print(logits)\n",
    "print('预测的label标签 {}'.format(label_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载模型\n",
    "model_path = './checkpoint/model_best03.pdparams'\n",
    "loaded_dict = paddle.load(model_path)\n",
    "model.load_dict(loaded_dict)\n",
    "model.eval()\n",
    "# 输入一条样本\n",
    "text_a = '电脑怎么录像?'\n",
    "text_b = '如何在计算机上录视频'\n",
    "texts = ['CLS'] + list(text_a) + ['SEP'] + list(text_b) + ['SEP']\n",
    "# text_a和text_b分别转换成id的形式\n",
    "input_ids_a = [word2id_dict[item] if item in word2id_dict else word2id_dict['[UNK]']\n",
    "               for item in text_a]\n",
    "input_ids_b = [word2id_dict[item] if item in word2id_dict else word2id_dict['[UNK]']\n",
    "               for item in text_b]\n",
    "# text_a 和text_b 拼接\n",
    "input_ids = [cls_id] + input_ids_a + [sep_id] + input_ids_b + [sep_id]\n",
    "# 分段编码的id的形式\n",
    "segment_ids = [0]*(len(input_ids_a)+2) + [1]*(len(input_ids_b)+1)\n",
    "print(\"输入的文本为：{}\".format(texts))\n",
    "print(\"输入的id形式：{}\".format(input_ids))\n",
    "# 转换成Tensor\n",
    "input_ids = paddle.to_tensor([input_ids])\n",
    "segment_ids = paddle.to_tensor([segment_ids])\n",
    "inputs = [input_ids, segment_ids]\n",
    "# 评估模式\n",
    "model.eval()\n",
    "# 模型预测\n",
    "with paddle.no_grad():\n",
    "    pooled_output = model(inputs)\n",
    "# 获取多头注意力权重\n",
    "atten_weights = model.attention_weight[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pandas as pd\n",
    "\n",
    "data_attention = atten_weights[0]\n",
    "plt.clf()\n",
    "font_size = 25\n",
    "font = FontProperties(fname='simhei.ttf', size=font_size)\n",
    "# 可视化其中的head，总共heads_num 个head\n",
    "for head in range(heads_num):\n",
    "    data = pd.DataFrame(data=data_attention[head], index=texts, columns=texts)\n",
    "    f, ax = plt.subplots(figsize=(13, 13))\n",
    "    # 使用heatmap可视化\n",
    "    sns.heatmap(data=data, ax=ax, cmap='OrRd', cbar=False)\n",
    "    # y轴旋转270度\n",
    "    label_y = ax.get_yticklabels()\n",
    "    plt.setp(label_y, rotation=270, horizontalalignment=\"right\", fontproperties=font)\n",
    "    # x轴旋转0度\n",
    "    label_x = ax.get_xticklabels()\n",
    "    plt.setp(label_x, rotation=0, horizontalalignment='right', fontproperties=font)\n",
    "    plt.savefig('att-vis3_{}.pdf'.format(head))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于框架API实现文本语义匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Transformer_v1(nn.Layer):\n",
    "    def __init__(self, vocab_size, n_block=1, hidden_size=768, heads_num=12, intermediate_size=3072, hidden_dropout=0.1,\n",
    "                 attention_dropout=0.1, act_dropout=0, position_size=512, num_classes=2, padding_idx=0):\n",
    "        super(Model_Transformer_v1, self).__init__()\n",
    "        # 词表大小\n",
    "        self.vocab_size = vocab_size\n",
    "        # Transformer的编码器数目\n",
    "        self.n_block = n_block\n",
    "        # 每个词映射成稠密向量的维度\n",
    "        self.hidden_size = hidden_size\n",
    "        # 多头注意力的个数\n",
    "        self.heads_num = heads_num\n",
    "        # 逐位前馈层的的维度\n",
    "        self.intermediate_size = intermediate_size\n",
    "        # Embedding层的 Dropout\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        # 多头注意力的dropout的 dropout参数\n",
    "        self.attention_dropout = attention_dropout\n",
    "        # 位置编码的大小 position_size\n",
    "        self.position_size = position_size\n",
    "        # 类别数\n",
    "        self.num_classes = num_classes\n",
    "        # 逐位前馈层的dropout\n",
    "        self.act_dropout = act_dropout\n",
    "        # [PAD]字符的ID\n",
    "        self.padding_idx = padding_idx\n",
    "        # 实例化输入编码，分段编码和位置编码\n",
    "        self.embeddings = TransformerEmbeddings(vocab_size=self.vocab_size, hidden_size=self.hidden_size,\n",
    "                                               hidden_dropout_prob=self.hidden_dropout, position_size=self.position_size)\n",
    "        # 实例化Transformer的编码器\n",
    "        self.layers = nn.LayerList([])\n",
    "        for i in range(n_block):\n",
    "            # 使用框架API\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=heads_num, dim_feedforward=intermediate_size,\n",
    "                                                       dropout=hidden_dropout, attn_dropout=attention_dropout, act_dropout=act_dropout)\n",
    "            self.layers.append(encoder_layer)\n",
    "        # 全连接层\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        # 双曲正切激活函数\n",
    "        self.activation = nn.Tanh()\n",
    "        # 最后一层分类器\n",
    "        self.classifier = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "\n",
    "    def forward(self, inputs, position_ids=None, attention_mask=None):\n",
    "        input_ids, segment_ids = inputs\n",
    "        # 构建Mask矩阵，把Pad的位置即input_ids中为0的位置设置为True,非0的位置设置为False\n",
    "        if attention_mask is None:\n",
    "            attention_mask = paddle.unsqueeze((input_ids == self.padding_idx).astype(\"float32\") * -1e9, axis=[1,2])\n",
    "        # 抽取特征向量\n",
    "        embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, segment_ids=segment_ids)\n",
    "        sequence_output = embedding_output\n",
    "        self._attention_weights = []\n",
    "        # Transformer的输出和注意力权重的输出\n",
    "        for i, encoder_layer in enumerate(self.layers):\n",
    "            sequence_output = encoder_layer(sequence_output, src_mask=attention_mask)\n",
    "        # 选择第0个位置的向量作为句向量\n",
    "        first_token_tensor = sequence_output[:,0]\n",
    "        # 输出层\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        # 句子级别的输出经过分类器\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "\n",
    "paddle.seed(2023)\n",
    "heads_num = 4\n",
    "epochs = 3\n",
    "vocab_size = 21128\n",
    "num_classes = 2\n",
    "# 注意力多头的数目\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 评估的时候采用准确率指标\n",
    "metric = Accuracy()\n",
    "# Transformer的分类器\n",
    "model = Model_Transformer_v1(vocab_size=vocab_size, n_block=1, num_classes=num_classes, heads_num=heads_num,\n",
    "                             padding_idx=padding_idx)\n",
    "# 排除所有的偏置和LayerNorm的参数\n",
    "decay_params = [p.name for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in ['bias', 'norm'])]\n",
    "\n",
    "# 定义Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=5E-5, parameters=model.parameters(), weight_decay=0.0,\n",
    "                                   apply_decay_param_fun=lambda x:x in decay_params)\n",
    "\n",
    "runner = RunnerV3(model=model, optimizer=optimizer, loss_fn=criterion, metric=metric)\n",
    "save_path = './checkpoint/model_best04.pdparams'\n",
    "runner.train(train_loader=train_loader, dev_loader=dev_loader, num_epochs=epochs, log_steps=100, eval_steps=500, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './checkpoint/model_best04.pdparams'\n",
    "runner.load_model(model_path=model_path)\n",
    "accuracy, _ = runner.evaluate(test_loader)\n",
    "# print(\"Evaluate on test set, Accuracy: {:.5f}\".format(accuracy))\n",
    "print(f\"Evaluate on test set, Accuracy: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加Transformer层数的实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] epoch: 0/3, step: 1300/22386, loss: 0.53706\n",
      "[Train] epoch: 0/3, step: 1400/22386, loss: 0.47577\n",
      "[Train] epoch: 0/3, step: 1500/22386, loss: 0.71870\n",
      "[Evaluate]  dev score: 0.58328, dev loss: 0.71074\n",
      "[Evaluate] best accuracy performence has been updated: 0.55987 --> 0.58328\n",
      "[Train] epoch: 0/3, step: 1600/22386, loss: 0.54842\n",
      "[Train] epoch: 0/3, step: 1700/22386, loss: 0.60748\n",
      "[Train] epoch: 0/3, step: 1800/22386, loss: 0.61626\n",
      "[Train] epoch: 0/3, step: 1900/22386, loss: 0.39963\n",
      "[Train] epoch: 0/3, step: 2000/22386, loss: 0.45947\n",
      "[Evaluate]  dev score: 0.58828, dev loss: 0.72194\n",
      "[Evaluate] best accuracy performence has been updated: 0.58328 --> 0.58828\n",
      "[Train] epoch: 0/3, step: 2100/22386, loss: 0.66097\n",
      "[Train] epoch: 0/3, step: 2200/22386, loss: 0.40756\n",
      "[Train] epoch: 0/3, step: 2300/22386, loss: 0.45504\n",
      "[Train] epoch: 0/3, step: 2400/22386, loss: 0.52569\n",
      "[Train] epoch: 0/3, step: 2500/22386, loss: 0.65277\n",
      "[Evaluate]  dev score: 0.61350, dev loss: 0.67520\n",
      "[Evaluate] best accuracy performence has been updated: 0.58828 --> 0.61350\n",
      "[Train] epoch: 0/3, step: 2600/22386, loss: 0.44345\n",
      "[Train] epoch: 0/3, step: 2700/22386, loss: 0.51793\n",
      "[Train] epoch: 0/3, step: 2800/22386, loss: 0.61584\n",
      "[Train] epoch: 0/3, step: 2900/22386, loss: 0.50969\n",
      "[Train] epoch: 0/3, step: 3000/22386, loss: 0.63408\n",
      "[Evaluate]  dev score: 0.61122, dev loss: 0.70647\n",
      "[Train] epoch: 0/3, step: 3100/22386, loss: 0.46031\n",
      "[Train] epoch: 0/3, step: 3200/22386, loss: 0.51417\n",
      "[Train] epoch: 0/3, step: 3300/22386, loss: 0.41215\n",
      "[Train] epoch: 0/3, step: 3400/22386, loss: 0.33721\n",
      "[Train] epoch: 0/3, step: 3500/22386, loss: 0.41403\n",
      "[Evaluate]  dev score: 0.63008, dev loss: 0.67904\n",
      "[Evaluate] best accuracy performence has been updated: 0.61350 --> 0.63008\n",
      "[Train] epoch: 0/3, step: 3600/22386, loss: 0.41568\n",
      "[Train] epoch: 0/3, step: 3700/22386, loss: 0.44747\n",
      "[Train] epoch: 0/3, step: 3800/22386, loss: 0.58715\n",
      "[Train] epoch: 0/3, step: 3900/22386, loss: 0.37527\n",
      "[Train] epoch: 0/3, step: 4000/22386, loss: 0.59830\n",
      "[Evaluate]  dev score: 0.62622, dev loss: 0.72200\n",
      "[Train] epoch: 0/3, step: 4100/22386, loss: 0.58154\n",
      "[Train] epoch: 0/3, step: 4200/22386, loss: 0.50468\n",
      "[Train] epoch: 0/3, step: 4300/22386, loss: 0.49324\n",
      "[Train] epoch: 0/3, step: 4400/22386, loss: 0.40740\n",
      "[Train] epoch: 0/3, step: 4500/22386, loss: 0.50965\n",
      "[Evaluate]  dev score: 0.63122, dev loss: 0.68181\n",
      "[Evaluate] best accuracy performence has been updated: 0.63008 --> 0.63122\n",
      "[Train] epoch: 0/3, step: 4600/22386, loss: 0.29213\n",
      "[Train] epoch: 0/3, step: 4700/22386, loss: 0.39361\n",
      "[Train] epoch: 0/3, step: 4800/22386, loss: 0.57218\n",
      "[Train] epoch: 0/3, step: 4900/22386, loss: 0.56271\n",
      "[Train] epoch: 0/3, step: 5000/22386, loss: 0.49863\n",
      "[Evaluate]  dev score: 0.64394, dev loss: 0.67505\n",
      "[Evaluate] best accuracy performence has been updated: 0.63122 --> 0.64394\n",
      "[Train] epoch: 0/3, step: 5100/22386, loss: 0.58372\n",
      "[Train] epoch: 0/3, step: 5200/22386, loss: 0.58970\n",
      "[Train] epoch: 0/3, step: 5300/22386, loss: 0.41967\n",
      "[Train] epoch: 0/3, step: 5400/22386, loss: 0.65631\n",
      "[Train] epoch: 0/3, step: 5500/22386, loss: 0.53345\n",
      "[Evaluate]  dev score: 0.63281, dev loss: 0.72458\n",
      "[Train] epoch: 0/3, step: 5600/22386, loss: 0.50592\n",
      "[Train] epoch: 0/3, step: 5700/22386, loss: 0.64718\n",
      "[Train] epoch: 0/3, step: 5800/22386, loss: 0.37323\n",
      "[Train] epoch: 0/3, step: 5900/22386, loss: 0.54942\n",
      "[Train] epoch: 0/3, step: 6000/22386, loss: 0.48620\n",
      "[Evaluate]  dev score: 0.64713, dev loss: 0.68141\n",
      "[Evaluate] best accuracy performence has been updated: 0.64394 --> 0.64713\n",
      "[Train] epoch: 0/3, step: 6100/22386, loss: 0.47045\n",
      "[Train] epoch: 0/3, step: 6200/22386, loss: 0.39936\n",
      "[Train] epoch: 0/3, step: 6300/22386, loss: 0.36002\n",
      "[Train] epoch: 0/3, step: 6400/22386, loss: 0.61930\n",
      "[Train] epoch: 0/3, step: 6500/22386, loss: 0.65069\n",
      "[Evaluate]  dev score: 0.64622, dev loss: 0.66776\n",
      "[Train] epoch: 0/3, step: 6600/22386, loss: 0.62045\n",
      "[Train] epoch: 0/3, step: 6700/22386, loss: 0.51530\n",
      "[Train] epoch: 0/3, step: 6800/22386, loss: 0.50884\n",
      "[Train] epoch: 0/3, step: 6900/22386, loss: 0.50478\n",
      "[Train] epoch: 0/3, step: 7000/22386, loss: 0.52757\n",
      "[Evaluate]  dev score: 0.66144, dev loss: 0.63952\n",
      "[Evaluate] best accuracy performence has been updated: 0.64713 --> 0.66144\n",
      "[Train] epoch: 0/3, step: 7100/22386, loss: 0.51002\n",
      "[Train] epoch: 0/3, step: 7200/22386, loss: 0.27523\n",
      "[Train] epoch: 0/3, step: 7300/22386, loss: 0.35038\n",
      "[Train] epoch: 0/3, step: 7400/22386, loss: 0.51827\n",
      "[Train] epoch: 1/3, step: 7500/22386, loss: 0.37524\n",
      "[Evaluate]  dev score: 0.65258, dev loss: 0.66135\n",
      "[Train] epoch: 1/3, step: 7600/22386, loss: 0.60492\n",
      "[Train] epoch: 1/3, step: 7700/22386, loss: 0.39668\n",
      "[Train] epoch: 1/3, step: 7800/22386, loss: 0.53885\n",
      "[Train] epoch: 1/3, step: 7900/22386, loss: 0.45662\n",
      "[Train] epoch: 1/3, step: 8000/22386, loss: 0.40548\n",
      "[Evaluate]  dev score: 0.65735, dev loss: 0.67841\n",
      "[Train] epoch: 1/3, step: 8100/22386, loss: 0.44670\n",
      "[Train] epoch: 1/3, step: 8200/22386, loss: 0.68075\n",
      "[Train] epoch: 1/3, step: 8300/22386, loss: 0.39040\n",
      "[Train] epoch: 1/3, step: 8400/22386, loss: 0.51877\n",
      "[Train] epoch: 1/3, step: 8500/22386, loss: 0.48075\n",
      "[Evaluate]  dev score: 0.65281, dev loss: 0.66023\n",
      "[Train] epoch: 1/3, step: 8600/22386, loss: 0.34514\n",
      "[Train] epoch: 1/3, step: 8700/22386, loss: 0.34722\n",
      "[Train] epoch: 1/3, step: 8800/22386, loss: 0.46923\n",
      "[Train] epoch: 1/3, step: 8900/22386, loss: 0.32221\n",
      "[Train] epoch: 1/3, step: 9000/22386, loss: 0.53649\n",
      "[Evaluate]  dev score: 0.66644, dev loss: 0.65873\n",
      "[Evaluate] best accuracy performence has been updated: 0.66144 --> 0.66644\n",
      "[Train] epoch: 1/3, step: 9100/22386, loss: 0.41541\n",
      "[Train] epoch: 1/3, step: 9200/22386, loss: 0.36365\n",
      "[Train] epoch: 1/3, step: 9300/22386, loss: 0.33586\n",
      "[Train] epoch: 1/3, step: 9400/22386, loss: 0.49941\n",
      "[Train] epoch: 1/3, step: 9500/22386, loss: 0.55422\n",
      "[Evaluate]  dev score: 0.65803, dev loss: 0.66807\n",
      "[Train] epoch: 1/3, step: 9600/22386, loss: 0.39501\n",
      "[Train] epoch: 1/3, step: 9700/22386, loss: 0.24949\n",
      "[Train] epoch: 1/3, step: 9800/22386, loss: 0.38928\n",
      "[Train] epoch: 1/3, step: 9900/22386, loss: 0.55248\n",
      "[Train] epoch: 1/3, step: 10000/22386, loss: 0.30265\n",
      "[Evaluate]  dev score: 0.67167, dev loss: 0.66212\n",
      "[Evaluate] best accuracy performence has been updated: 0.66644 --> 0.67167\n",
      "[Train] epoch: 1/3, step: 10100/22386, loss: 0.46640\n",
      "[Train] epoch: 1/3, step: 10200/22386, loss: 0.42846\n",
      "[Train] epoch: 1/3, step: 10300/22386, loss: 0.53407\n",
      "[Train] epoch: 1/3, step: 10400/22386, loss: 0.50247\n",
      "[Train] epoch: 1/3, step: 10500/22386, loss: 0.51619\n",
      "[Evaluate]  dev score: 0.68439, dev loss: 0.64960\n",
      "[Evaluate] best accuracy performence has been updated: 0.67167 --> 0.68439\n",
      "[Train] epoch: 1/3, step: 10600/22386, loss: 0.40058\n",
      "[Train] epoch: 1/3, step: 10700/22386, loss: 0.31785\n",
      "[Train] epoch: 1/3, step: 10800/22386, loss: 0.43624\n",
      "[Train] epoch: 1/3, step: 10900/22386, loss: 0.47210\n",
      "[Train] epoch: 1/3, step: 11000/22386, loss: 0.42790\n",
      "[Evaluate]  dev score: 0.68053, dev loss: 0.64793\n",
      "[Train] epoch: 1/3, step: 11100/22386, loss: 0.66922\n",
      "[Train] epoch: 1/3, step: 11200/22386, loss: 0.27810\n",
      "[Train] epoch: 1/3, step: 11300/22386, loss: 0.52067\n",
      "[Train] epoch: 1/3, step: 11400/22386, loss: 0.45217\n",
      "[Train] epoch: 1/3, step: 11500/22386, loss: 0.49237\n",
      "[Evaluate]  dev score: 0.65485, dev loss: 0.69894\n",
      "[Train] epoch: 1/3, step: 11600/22386, loss: 0.37694\n",
      "[Train] epoch: 1/3, step: 11700/22386, loss: 0.72386\n",
      "[Train] epoch: 1/3, step: 11800/22386, loss: 0.40666\n",
      "[Train] epoch: 1/3, step: 11900/22386, loss: 0.56117\n",
      "[Train] epoch: 1/3, step: 12000/22386, loss: 0.38099\n",
      "[Evaluate]  dev score: 0.67167, dev loss: 0.67017\n",
      "[Train] epoch: 1/3, step: 12100/22386, loss: 0.40155\n",
      "[Train] epoch: 1/3, step: 12200/22386, loss: 0.31819\n",
      "[Train] epoch: 1/3, step: 12300/22386, loss: 0.24587\n",
      "[Train] epoch: 1/3, step: 12400/22386, loss: 0.38578\n",
      "[Train] epoch: 1/3, step: 12500/22386, loss: 0.19938\n",
      "[Evaluate]  dev score: 0.68030, dev loss: 0.64922\n",
      "[Train] epoch: 1/3, step: 12600/22386, loss: 0.45185\n",
      "[Train] epoch: 1/3, step: 12700/22386, loss: 0.48376\n",
      "[Train] epoch: 1/3, step: 12800/22386, loss: 0.42734\n",
      "[Train] epoch: 1/3, step: 12900/22386, loss: 0.50555\n",
      "[Train] epoch: 1/3, step: 13000/22386, loss: 0.37688\n",
      "[Evaluate]  dev score: 0.68371, dev loss: 0.65480\n",
      "[Train] epoch: 1/3, step: 13100/22386, loss: 0.39858\n",
      "[Train] epoch: 1/3, step: 13200/22386, loss: 0.42586\n",
      "[Train] epoch: 1/3, step: 13300/22386, loss: 0.30976\n",
      "[Train] epoch: 1/3, step: 13400/22386, loss: 0.41282\n",
      "[Train] epoch: 1/3, step: 13500/22386, loss: 0.23677\n",
      "[Evaluate]  dev score: 0.65690, dev loss: 0.71043\n",
      "[Train] epoch: 1/3, step: 13600/22386, loss: 0.51237\n",
      "[Train] epoch: 1/3, step: 13700/22386, loss: 0.28823\n",
      "[Train] epoch: 1/3, step: 13800/22386, loss: 0.30958\n",
      "[Train] epoch: 1/3, step: 13900/22386, loss: 0.56946\n",
      "[Train] epoch: 1/3, step: 14000/22386, loss: 0.41041\n",
      "[Evaluate]  dev score: 0.69416, dev loss: 0.63701\n",
      "[Evaluate] best accuracy performence has been updated: 0.68439 --> 0.69416\n",
      "[Train] epoch: 1/3, step: 14100/22386, loss: 0.46028\n",
      "[Train] epoch: 1/3, step: 14200/22386, loss: 0.32629\n",
      "[Train] epoch: 1/3, step: 14300/22386, loss: 0.16312\n",
      "[Train] epoch: 1/3, step: 14400/22386, loss: 0.34374\n",
      "[Train] epoch: 1/3, step: 14500/22386, loss: 0.41598\n",
      "[Evaluate]  dev score: 0.68166, dev loss: 0.67679\n",
      "[Train] epoch: 1/3, step: 14600/22386, loss: 0.40607\n",
      "[Train] epoch: 1/3, step: 14700/22386, loss: 0.46144\n",
      "[Train] epoch: 1/3, step: 14800/22386, loss: 0.35790\n",
      "[Train] epoch: 1/3, step: 14900/22386, loss: 0.45794\n",
      "[Train] epoch: 2/3, step: 15000/22386, loss: 0.35427\n",
      "[Evaluate]  dev score: 0.69189, dev loss: 0.67061\n",
      "[Train] epoch: 2/3, step: 15100/22386, loss: 0.43972\n",
      "[Train] epoch: 2/3, step: 15200/22386, loss: 0.43863\n",
      "[Train] epoch: 2/3, step: 15300/22386, loss: 0.55458\n",
      "[Train] epoch: 2/3, step: 15400/22386, loss: 0.46276\n",
      "[Train] epoch: 2/3, step: 15500/22386, loss: 0.36440\n",
      "[Evaluate]  dev score: 0.68189, dev loss: 0.65870\n",
      "[Train] epoch: 2/3, step: 15600/22386, loss: 0.69951\n",
      "[Train] epoch: 2/3, step: 15700/22386, loss: 0.28611\n",
      "[Train] epoch: 2/3, step: 15800/22386, loss: 0.45495\n",
      "[Train] epoch: 2/3, step: 15900/22386, loss: 0.36895\n",
      "[Train] epoch: 2/3, step: 16000/22386, loss: 0.32212\n",
      "[Evaluate]  dev score: 0.68484, dev loss: 0.65254\n",
      "[Train] epoch: 2/3, step: 16100/22386, loss: 0.50211\n",
      "[Train] epoch: 2/3, step: 16200/22386, loss: 0.36097\n",
      "[Train] epoch: 2/3, step: 16300/22386, loss: 0.41665\n",
      "[Train] epoch: 2/3, step: 16400/22386, loss: 0.47575\n",
      "[Train] epoch: 2/3, step: 16500/22386, loss: 0.22291\n",
      "[Evaluate]  dev score: 0.69325, dev loss: 0.64632\n",
      "[Train] epoch: 2/3, step: 16600/22386, loss: 0.37321\n",
      "[Train] epoch: 2/3, step: 16700/22386, loss: 0.27164\n",
      "[Train] epoch: 2/3, step: 16800/22386, loss: 0.33507\n",
      "[Train] epoch: 2/3, step: 16900/22386, loss: 0.35800\n",
      "[Train] epoch: 2/3, step: 17000/22386, loss: 0.48038\n",
      "[Evaluate]  dev score: 0.67939, dev loss: 0.67134\n",
      "[Train] epoch: 2/3, step: 17100/22386, loss: 0.35700\n",
      "[Train] epoch: 2/3, step: 17200/22386, loss: 0.29792\n",
      "[Train] epoch: 2/3, step: 17300/22386, loss: 0.37359\n",
      "[Train] epoch: 2/3, step: 17400/22386, loss: 0.42564\n",
      "[Train] epoch: 2/3, step: 17500/22386, loss: 0.36659\n",
      "[Evaluate]  dev score: 0.70870, dev loss: 0.60556\n",
      "[Evaluate] best accuracy performence has been updated: 0.69416 --> 0.70870\n",
      "[Train] epoch: 2/3, step: 17600/22386, loss: 0.52190\n",
      "[Train] epoch: 2/3, step: 17700/22386, loss: 0.32521\n",
      "[Train] epoch: 2/3, step: 17800/22386, loss: 0.43808\n",
      "[Train] epoch: 2/3, step: 17900/22386, loss: 0.36175\n",
      "[Train] epoch: 2/3, step: 18000/22386, loss: 0.45995\n",
      "[Evaluate]  dev score: 0.70052, dev loss: 0.59793\n",
      "[Train] epoch: 2/3, step: 18100/22386, loss: 0.21646\n",
      "[Train] epoch: 2/3, step: 18200/22386, loss: 0.39796\n",
      "[Train] epoch: 2/3, step: 18300/22386, loss: 0.42758\n",
      "[Train] epoch: 2/3, step: 18400/22386, loss: 0.46828\n",
      "[Train] epoch: 2/3, step: 18500/22386, loss: 0.32048\n",
      "[Evaluate]  dev score: 0.71007, dev loss: 0.60447\n",
      "[Evaluate] best accuracy performence has been updated: 0.70870 --> 0.71007\n",
      "[Train] epoch: 2/3, step: 18600/22386, loss: 0.26973\n",
      "[Train] epoch: 2/3, step: 18700/22386, loss: 0.25525\n",
      "[Train] epoch: 2/3, step: 18800/22386, loss: 0.54655\n",
      "[Train] epoch: 2/3, step: 18900/22386, loss: 0.29966\n",
      "[Train] epoch: 2/3, step: 19000/22386, loss: 0.38369\n",
      "[Evaluate]  dev score: 0.71575, dev loss: 0.60512\n",
      "[Evaluate] best accuracy performence has been updated: 0.71007 --> 0.71575\n",
      "[Train] epoch: 2/3, step: 19100/22386, loss: 0.45536\n",
      "[Train] epoch: 2/3, step: 19200/22386, loss: 0.37732\n",
      "[Train] epoch: 2/3, step: 19300/22386, loss: 0.43890\n",
      "[Train] epoch: 2/3, step: 19400/22386, loss: 0.25217\n",
      "[Train] epoch: 2/3, step: 19500/22386, loss: 0.30033\n",
      "[Evaluate]  dev score: 0.70711, dev loss: 0.63302\n",
      "[Train] epoch: 2/3, step: 19600/22386, loss: 0.36976\n",
      "[Train] epoch: 2/3, step: 19700/22386, loss: 0.42078\n",
      "[Train] epoch: 2/3, step: 19800/22386, loss: 0.29575\n",
      "[Train] epoch: 2/3, step: 19900/22386, loss: 0.36233\n",
      "[Train] epoch: 2/3, step: 20000/22386, loss: 0.58317\n",
      "[Evaluate]  dev score: 0.69189, dev loss: 0.64743\n",
      "[Train] epoch: 2/3, step: 20100/22386, loss: 0.47239\n",
      "[Train] epoch: 2/3, step: 20200/22386, loss: 0.36576\n",
      "[Train] epoch: 2/3, step: 20300/22386, loss: 0.32611\n",
      "[Train] epoch: 2/3, step: 20400/22386, loss: 0.50930\n",
      "[Train] epoch: 2/3, step: 20500/22386, loss: 0.37232\n",
      "[Evaluate]  dev score: 0.69325, dev loss: 0.63625\n",
      "[Train] epoch: 2/3, step: 20600/22386, loss: 0.49728\n",
      "[Train] epoch: 2/3, step: 20700/22386, loss: 0.47676\n",
      "[Train] epoch: 2/3, step: 20800/22386, loss: 0.42056\n",
      "[Train] epoch: 2/3, step: 20900/22386, loss: 0.35382\n",
      "[Train] epoch: 2/3, step: 21000/22386, loss: 0.30304\n",
      "[Evaluate]  dev score: 0.68053, dev loss: 0.66831\n",
      "[Train] epoch: 2/3, step: 21100/22386, loss: 0.37765\n",
      "[Train] epoch: 2/3, step: 21200/22386, loss: 0.24489\n",
      "[Train] epoch: 2/3, step: 21300/22386, loss: 0.48940\n",
      "[Train] epoch: 2/3, step: 21400/22386, loss: 0.40252\n",
      "[Train] epoch: 2/3, step: 21500/22386, loss: 0.25614\n",
      "[Evaluate]  dev score: 0.70370, dev loss: 0.61676\n",
      "[Train] epoch: 2/3, step: 21600/22386, loss: 0.29953\n",
      "[Train] epoch: 2/3, step: 21700/22386, loss: 0.37866\n",
      "[Train] epoch: 2/3, step: 21800/22386, loss: 0.42885\n",
      "[Train] epoch: 2/3, step: 21900/22386, loss: 0.29956\n",
      "[Train] epoch: 2/3, step: 22000/22386, loss: 0.45340\n",
      "[Evaluate]  dev score: 0.70711, dev loss: 0.61063\n",
      "[Train] epoch: 2/3, step: 22100/22386, loss: 0.45623\n",
      "[Train] epoch: 2/3, step: 22200/22386, loss: 0.27899\n",
      "[Train] epoch: 2/3, step: 22300/22386, loss: 0.34010\n",
      "[Evaluate]  dev score: 0.71007, dev loss: 0.59639\n",
      "[Train] Training done!\n"
     ]
    }
   ],
   "source": [
    "paddle.seed(2023)\n",
    "heads_num = 4\n",
    "epochs = 3\n",
    "vocab_size = 21128\n",
    "num_classes = 2\n",
    "# 注意力多头的数目\n",
    "# 交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 评估的时候采用准确率指标\n",
    "metric = Accuracy()\n",
    "# Transformer的分类模型\n",
    "model = Model_Transformer_v1(vocab_size=vocab_size, n_block=2, num_classes=num_classes, heads_num=heads_num, padding_idx=padding_idx)\n",
    "# 排除所有的偏置和LayerNorm的参数\n",
    "decay_params = [p.name for n,p in model.named_parameters()\n",
    "                if not any(nd in n for nd in ['bias', 'norm'])]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=5E-5, parameters=model.parameters(), weight_decay=0.0,\n",
    "                                   apply_decay_param_fun=lambda x:x in decay_params)\n",
    "\n",
    "runner = RunnerV3(model=model, optimizer=optimizer, loss_fn=criterion, metric=metric)\n",
    "save_path = './checkpoint/model_best05.pdparams'\n",
    "runner.train(train_loader=train_loader, dev_loader=dev_loader, num_epochs=epochs, log_steps=100,\n",
    "             eval_steps=500, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set, Accuracy: 0.70688\n"
     ]
    }
   ],
   "source": [
    "model_path = './checkpoint/model_best05.pdparams'\n",
    "runner.load_model(model_path=model_path)\n",
    "accuracy, _ = runner.evaluate(test_loader)\n",
    "print(f\"Evaluate on test set, Accuracy: {accuracy:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
